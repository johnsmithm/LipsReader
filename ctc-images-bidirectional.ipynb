{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from six.moves import xrange as range\n",
    "from python_speech_features import mfcc\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "\n",
    "import random\n",
    "import scipy\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq[0]), range(len(seq[0]))))\n",
    "        values.extend(seq[0])\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape\n",
    "\n",
    "# Loading the data\n",
    "# Constants\n",
    "SPACE_TOKEN = '<space>'\n",
    "SPACE_INDEX = 0\n",
    "FIRST_INDEX = ord('a') - 1  # 0 is reserved to space\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#s = getBatch(1,3,['test/video-2-train.mat','test/audio-2-train.mat','test/align-2-train.mat'])\n",
    "\n",
    "\n",
    "#print(np.asarray([s[0][0]]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,name=\"v\"):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial,name=name+\"_weight\")\n",
    "\n",
    "def bias_variable(shape,name=\"v\"):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name=name+\"_bias\")\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv3d(x, W, strides=[1, 1, 1, 1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool3d(x, ksize=[1,1, 2, 2, 1],\n",
    "                        strides=[1,1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Some configs\n",
    "\n",
    "# Accounting the 0th indice +  space + blank label = 28 characters\n",
    "num_classes = ord('z') - ord('a') + 1 + 1 + 1\n",
    "\n",
    "# Hyper-parameters\n",
    "\n",
    "num_hidden = 100\n",
    "num_layers = 2\n",
    "batch_size = 1\n",
    "initial_learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "height, width = 50,80\n",
    "num_features = height* width*3\n",
    "\n",
    "\n",
    "\n",
    "# THE MAIN CODE!\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # e.g: log filter bank or MFCC features\n",
    "    # Has size [batch_size, max_stepsize, num_features], but the\n",
    "    # batch_size and max_stepsize can vary along each step\n",
    "    #inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "    \n",
    "    \n",
    "    #batch, steps-75, heigth*width-50*120\n",
    "    videoInputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "    shapeV = tf.shape(videoInputs)\n",
    "    \n",
    "    W_conv1 = weight_variable([3, 5,5, 3, 20],\"l1\")\n",
    "    b_conv1 = bias_variable([20],\"l1\")\n",
    "    #size (batch*steps, heigth, width,1)\n",
    "    x_image = tf.reshape(videoInputs , [-1,75,height, width,3])\n",
    "    #shape (batch*steps, heigth, width,32)\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    #shape (batch*steps, heigth/2=25, width/2=30,32)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    W_conv2 = weight_variable([3, 5,5, 20, 40],\"l2\")\n",
    "    b_conv2 = bias_variable([40],\"l2\")\n",
    "    #shape (batch*steps, 25,30,64)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    #shape (batch*steps, 13,15,64)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)  \n",
    "    \n",
    "    W_conv3 = weight_variable([3, 5,5, 40, 60],\"l3\")\n",
    "    b_conv3 = bias_variable([60],\"l3\")\n",
    "    #shape (batch*steps, 25,30,64)\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    #shape (batch*steps, 13,15,64)\n",
    "    h_pool3 = max_pool_2x2(h_conv3) \n",
    "    \n",
    "    hh = int(round(round(round(height/2)/2)/2))\n",
    "    ww = int(round(round(round(width/2)/2)/2))\n",
    "    \n",
    "    \n",
    "    #W_fc1 = weight_variable([hh*ww*60, 104],name=\"w1\")#41216 based on heigth and weith\n",
    "    #b_fc1 = bias_variable([104],name=\"b1\")\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool3, [-1,75, hh*ww*60])\n",
    "    #h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    #h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    #h_pool2_flat = tf.reshape(h_fc1_drop, [shapeV[0],shapeV[1], 104])   \n",
    "    \n",
    "    \n",
    "    # Here we use sparse_placeholder that will generate a\n",
    "    # SparseTensor required by ctc_loss op.\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "\n",
    "    # 1d array of size [batch_size]\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    # Defining the cell\n",
    "    # Can be:\n",
    "    #   tf.nn.rnn_cell.RNNCell\n",
    "    #   tf.nn.rnn_cell.GRUCell\n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_hidden)#, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.9)\n",
    "    \"\"\"\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell] * 4, state_is_tuple=True)\n",
    "    \n",
    "    outputs, states  = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell,\n",
    "        cell_bw=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=X_lengths,\n",
    "        inputs=X)\n",
    "\n",
    "    output_fw, output_bw = outputs\n",
    "    states_fw, states_bw = states\n",
    "    \"\"\"\n",
    "\n",
    "    # Stacking rnn cells\n",
    "    stackf = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers,\n",
    "                                        state_is_tuple=True)\n",
    "    stackb = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers,\n",
    "                                        state_is_tuple=True)\n",
    "\n",
    "    # The second output is the last state and we will no use that\n",
    "    #outputs, _ = tf.nn.dynamic_rnn(stack, h_pool2_flat, seq_len, dtype=tf.float32)\n",
    "\n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(h_pool2_flat, [1, 0, 2])\n",
    "    # Reshape to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, hh*ww*60])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, 75, x)\n",
    "\n",
    "    # The second output is the last state and we will no use that\n",
    "    #outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "    #try:\n",
    "    outputs1, _, _ = tf.nn.bidirectional_rnn(stackf, stackb, x, sequence_length=seq_len,\n",
    "                                              dtype=tf.float32)\n",
    "    \n",
    "    batch_s, max_timesteps = shapeV[0],shapeV[1]\n",
    "\n",
    "    # Reshaping to apply the same weights over the timesteps\n",
    "    outputs = tf.reshape(outputs1, [-1, 2*num_hidden])\n",
    "\n",
    "    # Truncated normal with mean 0 and stdev=0.1\n",
    "    # Tip: Try another initialization\n",
    "    # see https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.layers.html#initializers\n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden*2,\n",
    "                                         num_classes],\n",
    "                                        stddev=0.1))\n",
    "    # Zero initialization\n",
    "    # Tip: Is tf.zeros_initializer the same?\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "\n",
    "    # Doing the affine projection\n",
    "    logits = tf.matmul(outputs, W) +  b \n",
    "    \n",
    "    #keep_prob = tf.placeholder(tf.float32)\n",
    "    logits = tf.nn.dropout(logits, keep_prob)\n",
    "\n",
    "    # Reshaping back to the original shape\n",
    "    logits = tf.reshape(logits, [ -1, batch_s, num_classes])\n",
    "    \n",
    "    \n",
    "    # Time major\n",
    "    #logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "    loss = ctc_ops.ctc_loss(logits, targets, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=initial_learning_rate).minimize(cost)\n",
    "    #optimizer = tf.train.MomentumOptimizer(initial_learning_rate,  0.9).minimize(cost)\n",
    "\n",
    "    # Option 2: tf.contrib.ctc.ctc_beam_search_decoder\n",
    "    # (it's slower but you'll get better results)\n",
    "    #decoded, log_prob = ctc_ops.ctc_greedy_decoder(logits, seq_len)\n",
    "    decoded, log_prob = ctc_ops.ctc_beam_search_decoder(logits, seq_len)\n",
    "    \n",
    "    # Inaccuracy: label error rate\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "                                          targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75 75]\n"
     ]
    }
   ],
   "source": [
    "def makeSameLengthO(ar):\n",
    "    m = 0\n",
    "    for k,i in enumerate(ar):\n",
    "        m = max(m,i.shape[0])\n",
    "    at = []\n",
    "    for k,i in enumerate(ar):\n",
    "        if i.shape[0]!=m:            \n",
    "            ar[k] = np.insert(ar[k], [i.shape[0]]*(m-i.shape[0]), 0., axis=0)\n",
    "            at.append(ar[k])\n",
    "        else:\n",
    "            at.append(ar[k])\n",
    "    return np.asarray(at)\n",
    "\n",
    "def getBatchO(batchSize, epoch=0, fileNames=False,randomIds=False, database=False,saveAll=False, fromFile=False):\n",
    "    #what is better, to save in memory or to read from file every time>>??\n",
    "    #openfile    \n",
    "    if fileNames != False:\n",
    "        video=scipy.io.loadmat(fileNames[0])['video']\n",
    "        audio=scipy.io.loadmat(fileNames[1])['audio'][0]#must be same length-get max length, and add zeros to others\n",
    "        align=scipy.io.loadmat(fileNames[2])['align'][0]\n",
    "        lengthA=scipy.io.loadmat(fileNames[1])['lengths'][0]\n",
    "        lengthV=scipy.io.loadmat(fileNames[0])['lengths'][0]\n",
    "        #audio, lengths = makeSameLength(audio)\n",
    "        database = [audio,align,video,lengthA,lengthV]\n",
    "    \n",
    "    assert batchSize < len(database[0])\n",
    "    #print(len(database[0]))\n",
    "    mask = np.ones(len(database[0]),dtype=bool)\n",
    "    if randomIds:\n",
    "        b = random.sample(range(0, len(database[0])), batchSize)        \n",
    "    else:\n",
    "        offset = (batchSize*epoch)%len(database[0])\n",
    "        b = np.zeros(batchSize,dtype=np.int32)\n",
    "        b[:min(offset+batchSize,len(database[0]))-offset] = np.arange(offset,min(offset+batchSize,len(database[0]))) \n",
    "        if offset+batchSize > len(database[0]):\n",
    "            b[min(offset+batchSize,len(database[0]))-offset:] = np.arange(offset+batchSize-len(database[0]),dtype=np.int32)\n",
    "    #print(b)\n",
    "    mask[b] = False    \n",
    "    data = [False,sparse_tuple_from(database[1][~mask]),\n",
    "            database[2][~mask],database[3][~mask],database[4][~mask]]  \n",
    "    #return and correct audio length\n",
    "    return data\n",
    "\n",
    "def getVideoBatch(batchSize, nr, path):\n",
    "    t=0\n",
    "    for f1 in sorted(glob.glob(path+\"/*.mat\")):        \n",
    "        if f1.find('video')!=-1 and f1.find('words')==-1:\n",
    "            t=t+1\n",
    "    nr = nr % t\n",
    "    \n",
    "    for f1 in sorted(glob.glob(path+\"/*.mat\")):        \n",
    "        if f1.find('video')!=-1 and f1.find('words')==-1:\n",
    "            if nr != 0:\n",
    "                nr = nr -1\n",
    "                continue\n",
    "            f2 = f1.replace('video','align')            \n",
    "            #print(f1)\n",
    "            #print(f2)\n",
    "            video=scipy.io.loadmat(f1)['video']\n",
    "            align=scipy.io.loadmat(f2)['align'][0]\n",
    "            lengthA=scipy.io.loadmat(f2)['lengths'][0]\n",
    "            lengthV=scipy.io.loadmat(f1)['lengths'][0]\n",
    "            if len(video)<=batchSize  :\n",
    "                return [video, lengthV,sparse_tuple_from(align)]\n",
    "            b = random.sample(range(0, len(video)), batchSize)\n",
    "            mask = np.ones(len(video),dtype=bool)\n",
    "            mask[b] = False\n",
    "            return [video[~mask], lengthV[~mask],sparse_tuple_from(align[~mask])]\n",
    "#s = getBatchO(2,8,['test/video-2-train5.mat','test/audio-2-train5.mat','test/align-2-train.mat'],randomIds=True)\n",
    "\n",
    "s  = getVideoBatch(2,2,\"test/batch-3-50/\")\n",
    "print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 600\n",
    "display_step = 10\n",
    "batch_size = 2\n",
    "num_examples = 2\n",
    "num_batches_per_epoch = int(num_examples/batch_size)\n",
    "restore, save = \"models/images-100ep-10bs-100ex-2CL.ckpt\",\"models/images-200ep-10bs-100ex-2CL.ckpt\"#.ckpt\n",
    "restore, save = False,False#\"models/images-bgru-2ep-10bs-100ex-2CLstd-direct-3d.ckpt\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Initializate the weights and biases\n",
    "    saver = tf.train.Saver()\n",
    "    if restore:\n",
    "        saver.restore(session, restore)\n",
    "        print(\"Model restored.\",restore)\n",
    "    else:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "    for curr_epoch in range(num_epochs):\n",
    "        train_cost = train_ler = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in range(num_batches_per_epoch):\n",
    "            s = getVideoBatch(batch_size,batch,\n",
    "                         \"test/batch-3-50/\")\n",
    "            feed = {videoInputs: s[0],\n",
    "                    targets: s[2],\n",
    "                    seq_len: s[1],keep_prob:0.9}\n",
    "            batch_cost, _ = session.run([cost, optimizer], feed)\n",
    "            train_cost += batch_cost*batch_size\n",
    "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
    "        if curr_epoch % display_step == 0:\n",
    "            train_cost /= num_examples\n",
    "            train_ler /= num_examples\n",
    "            s1 = getVideoBatch(2,0,\n",
    "                          \"test/batch-3-50/\")\n",
    "            val_feed = {videoInputs: s1[0],\n",
    "                        targets: s1[2],\n",
    "                        seq_len: s1[1],keep_prob:0.9}\n",
    "\n",
    "            val_cost, val_ler = session.run([cost, ler], feed_dict=val_feed)\n",
    "\n",
    "            log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}\"\n",
    "            print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler,\n",
    "                             val_cost, val_ler, time.time() - start))\n",
    "            \n",
    "            if save and curr_epoch%100==0 and curr_epoch>0:\n",
    "                save_path = saver.save(session, save)\n",
    "                print(\"model saved:\",save_path)\n",
    "    # Decoding\n",
    "    d = session.run(decoded[0], feed_dict=val_feed)\n",
    "    str_decoded = ''.join([chr(x) for x in np.asarray(d[1]) + FIRST_INDEX])\n",
    "    # Replacing blank label to none\n",
    "    str_decoded = str_decoded.replace(chr(ord('z') + 1), '')\n",
    "    # Replacing space label to space\n",
    "    str_decoded = str_decoded.replace(chr(ord('a') - 1), ' ')\n",
    "    \n",
    "    if save:\n",
    "        save_path = saver.save(session, save)\n",
    "        print(\"model saved:\",save_path)\n",
    "\n",
    "    #print('Original:\\n%s' % original)\n",
    "    print('Decoded:\\n%s' % str_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
